{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.imports import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.layers import *\n",
    "from local.data.pipeline import *\n",
    "from local.data.source import *\n",
    "from local.data.core import *\n",
    "from local.data.external import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.optimizer import *\n",
    "from local.learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp callback.hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model hooks\n",
    "\n",
    "> Callback and helper function to add hooks in models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following for testing purposes (a basic linear regression problem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def synth_data(a=2, b=3, bs=16, n_trn=10, n_val=2):\n",
    "    x_trn = torch.randn(bs*n_trn, 1)\n",
    "    y_trn = a*x_trn + b + 0.1*torch.randn(bs*n_trn, 1)\n",
    "    x_val = torch.randn(bs*n_val, 1)\n",
    "    y_val = a*x_val + b + 0.1*torch.randn(bs*n_val, 1)\n",
    "    train_ds = TensorDataset(x_trn, y_trn)\n",
    "    valid_ds = TensorDataset(x_val, y_val)\n",
    "    train_dl = TfmdDL(train_ds, bs=bs, shuffle=True)\n",
    "    valid_dl = TfmdDL(valid_ds, bs=bs)\n",
    "    return DataBunch(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))\n",
    "    def forward(self, x): return x * self.a + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_learner(n_trn=10, n_val=2, **kwargs):\n",
    "    return Learner(RegModel(), synth_data(n_trn=n_trn,n_val=n_val), MSELossFlat(), opt_func=partial(SGD, mom=0.9), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are hooks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooks are function you can attach to a particular layer in your model and that will be executed in the foward pass (for forward hooks) or backward pass (for backward hooks).\n",
    "\n",
    "Forward hooks are functions that take three arguments, the layer it's applied to, the input of that layer and the output of that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=3, bias=True) (tensor([[ 1.8137, -0.3405, -1.8038, -1.2162,  3.0698],\n",
      "        [-0.3961, -0.1697,  0.1833, -0.9861, -0.0687],\n",
      "        [ 2.2365,  2.4447, -1.3131,  0.4555,  1.7607],\n",
      "        [ 0.7440, -0.1006, -1.2119,  0.6596,  1.0360]]),) tensor([[ 2.5172,  0.7680, -0.0755],\n",
      "        [-0.1359, -0.2236,  0.1336],\n",
      "        [ 1.6839,  0.9853, -0.0560],\n",
      "        [ 1.2432,  0.6146,  0.3685]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "tst_model = nn.Linear(5,3)\n",
    "def example_forward_hook(m,i,o): print(m,i,o)\n",
    "    \n",
    "x = torch.randn(4,5)\n",
    "hook = tst_model.register_forward_hook(example_forward_hook)\n",
    "y = tst_model(x)\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward hooks are functions that take three arguments: the layer it's applied to, the gradients of the loss with respect to the input, and the gradients with respect to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=3, bias=True) (tensor([-0.0441,  0.3203, -0.1670]), None, tensor([[ 0.0263,  0.1888, -0.0427],\n",
      "        [-0.2727,  0.2828, -0.1494],\n",
      "        [-0.3649,  0.3190, -0.2619],\n",
      "        [-0.3129,  0.0943,  0.0154],\n",
      "        [ 0.4231, -0.1366, -0.0128]])) (tensor([[-0.0960,  0.1345, -0.0605],\n",
      "        [-0.1369,  0.0697, -0.0723],\n",
      "        [ 0.1337,  0.0700, -0.0641],\n",
      "        [ 0.0551,  0.0461,  0.0299]]),)\n"
     ]
    }
   ],
   "source": [
    "def example_backward_hook(m,gi,go): print(m,gi,go)\n",
    "hook = tst_model.register_backward_hook(example_backward_hook)\n",
    "\n",
    "x = torch.randn(4,5)\n",
    "y = tst_model(x)\n",
    "loss = y.pow(2).mean()\n",
    "loss.backward()\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooks can change the input/output of a layer, or the gradients, print values or shapes. If you want to store something related to theses inputs/outputs, it's best to have you hook associated to a class so that it can put it in the state of an instance of that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@docs\n",
    "class Hook():\n",
    "    \"Create a hook on `m` with `hook_func`.\"\n",
    "    def __init__(self, m, hook_func, is_forward=True, detach=True, cpu=False):\n",
    "        self.hook_func,self.detach,self.cpu,self.stored = hook_func,detach,cpu,None\n",
    "        f = m.register_forward_hook if is_forward else m.register_backward_hook\n",
    "        self.hook = f(self.hook_fn)\n",
    "        self.removed = False\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        \"Applies `hook_func` to `module`, `input`, `output`.\"\n",
    "        if self.detach: input,output = to_detach(input, cpu=self.cpu),to_detach(output, cpu=self.cpu)\n",
    "        self.stored = self.hook_func(module, input, output)\n",
    "\n",
    "    def remove(self):\n",
    "        \"Remove the hook from the model.\"\n",
    "        if not self.removed:\n",
    "            self.hook.remove()\n",
    "            self.removed=True\n",
    "\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__(self, *args): self.remove()\n",
    "        \n",
    "    _docs = dict(__enter__=\"Register the hook\",\n",
    "                 __exit__=\"Remove the hook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be called during the forward pass if `is_forward=True`, the backward pass otherwise, and will optionally `detach` and put on the `cpu` the (gradient of the) input/output of the model before passing them to `hook_func`. The result of `hook_func` will be stored in the `stored` attribute of the `Hook`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_model = nn.Linear(5,3)\n",
    "hook = Hook(tst_model, lambda m,i,o: o)\n",
    "y = tst_model(x)\n",
    "test_eq(hook.stored, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>Hook.hook_fn</code>\" class=\"doc_header\"><code>Hook.hook_fn</code><a href=\"https://github.com/fastai/fastai_docs/tree/master/dev/__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Hook.hook_fn</code>(**`module`**, **`input`**, **`output`**)\n",
       "\n",
       "Applies `hook_func` to `module`, `input`, `output`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Hook.hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>Hook.remove</code>\" class=\"doc_header\"><code>Hook.remove</code><a href=\"https://github.com/fastai/fastai_docs/tree/master/dev/__main__.py#L15\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Hook.remove</code>()\n",
       "\n",
       "Remove the hook from the model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Hook.remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: It's important to properly remove your hooks for your model when you're done to avoid them being called again next time your model is applied to some inputs, and to free the memory that go with their state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_model = nn.Linear(5,10)\n",
    "x = torch.randn(4,5)\n",
    "y = tst_model(x)\n",
    "hook = Hook(tst_model, example_forward_hook)\n",
    "test_stdout(lambda: tst_model(x), f\"{tst_model} [{x}] {y.detach()}\")\n",
    "hook.remove()\n",
    "test_stdout(lambda: tst_model(x), \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's very important to remove your `Hook` even if your code is interrupted by some bug, `Hook` can be used as context managers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>Hook.__enter__</code>\" class=\"doc_header\"><code>Hook.__enter__</code><a href=\"https://github.com/fastai/fastai_docs/tree/master/dev/__main__.py#L21\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Hook.__enter__</code>(**\\*`args`**)\n",
       "\n",
       "Register the hook"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Hook.__enter__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>Hook.__exit__</code>\" class=\"doc_header\"><code>Hook.__exit__</code><a href=\"https://github.com/fastai/fastai_docs/tree/master/dev/__main__.py#L22\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Hook.__exit__</code>(**\\*`args`**)\n",
       "\n",
       "Remove the hook"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Hook.__exit__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_model = nn.Linear(5,10)\n",
    "x = torch.randn(4,5)\n",
    "y = tst_model(x)\n",
    "with Hook(tst_model, example_forward_hook) as h:\n",
    "    test_stdout(lambda: tst_model(x), f\"{tst_model} [{x}] {y.detach()}\")\n",
    "test_stdout(lambda: tst_model(x), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hook_inner(m,i,o): return o if isinstance(o,Tensor) or is_listy(o) else list(o)\n",
    "\n",
    "def hook_output(module, detach=True, cpu=False, grad=False):\n",
    "    \"Return a `Hook` that stores activations of `module` in `self.stored`\"\n",
    "    return Hook(module, _hook_inner, detach=detach, cpu=cpu, is_forward=not grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activations stored are the gradients if `grad=True`, otherwise the output of `module`. If `detach=True` they are detached from their history, and if `cpu=True`, they're put on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_model = nn.Linear(5,10)\n",
    "x = torch.randn(4,5)\n",
    "with hook_output(tst_model) as h:\n",
    "    y = tst_model(x)\n",
    "    test_eq(y, h.stored)\n",
    "    assert not h.stored.requires_grad\n",
    "    \n",
    "with hook_output(tst_model, grad=True) as h:\n",
    "    y = tst_model(x)\n",
    "    loss = y.pow(2).mean()\n",
    "    loss.backward()\n",
    "    test_close(2*y / y.numel(), h.stored[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda\n",
    "with hook_output(tst_model, cpu=True) as h:\n",
    "    y = tst_model.cuda()(x.cuda())\n",
    "    test_eq(h.stored.device, torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@docs\n",
    "class Hooks():\n",
    "    \"Create several hooks on the modules in `ms` with `hook_func`.\"\n",
    "    def __init__(self, ms, hook_func, is_forward=True, detach=True, cpu=False):\n",
    "        self.hooks = [Hook(m, hook_func, is_forward, detach, cpu) for m in ms]\n",
    "\n",
    "    def __getitem__(self,i): return self.hooks[i]\n",
    "    def __len__(self):       return len(self.hooks)\n",
    "    def __iter__(self):      return iter(self.hooks)\n",
    "    @property\n",
    "    def stored(self):        return [o.stored for o in self]\n",
    "\n",
    "    def remove(self):\n",
    "        \"Remove the hooks from the model.\"\n",
    "        for h in self.hooks: h.remove()\n",
    "\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__ (self, *args): self.remove()\n",
    "            \n",
    "    _docs = dict(stored = \"The states saved in each hook.\",\n",
    "                 __enter__=\"Register the hooks\",\n",
    "                 __exit__=\"Remove the hooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(5,10), nn.ReLU(), nn.Linear(10,3)]\n",
    "tst_model = nn.Sequential(*layers)\n",
    "hooks = Hooks(tst_model, lambda m,i,o: o)\n",
    "y = tst_model(x)\n",
    "test_eq(hooks.stored[0], layers[0](x))\n",
    "test_eq(hooks.stored[1], F.relu(layers[0](x)))\n",
    "test_eq(hooks.stored[2], y)\n",
    "hooks.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>Hooks.stored</code>\" class=\"doc_header\"><code>Hooks.stored</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "The states saved in each hook."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Hooks.stored, name='Hooks.stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>Hooks.remove</code>\" class=\"doc_header\"><code>Hooks.remove</code><a href=\"https://github.com/fastai/fastai_docs/tree/master/dev/__main__.py#L13\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Hooks.remove</code>()\n",
       "\n",
       "Remove the hooks from the model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Hooks.remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `Hook` , you can use `Hooks` as context managers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>Hooks.__enter__</code>\" class=\"doc_header\"><code>Hooks.__enter__</code><a href=\"https://github.com/fastai/fastai_docs/tree/master/dev/__main__.py#L17\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Hooks.__enter__</code>(**\\*`args`**)\n",
       "\n",
       "Register the hooks"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Hooks.__enter__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>Hooks.__exit__</code>\" class=\"doc_header\"><code>Hooks.__exit__</code><a href=\"https://github.com/fastai/fastai_docs/tree/master/dev/__main__.py#L18\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Hooks.__exit__</code>(**\\*`args`**)\n",
       "\n",
       "Remove the hooks"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Hooks.__exit__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(5,10), nn.ReLU(), nn.Linear(10,3)]\n",
    "tst_model = nn.Sequential(*layers)\n",
    "with Hooks(layers, lambda m,i,o: o) as h:\n",
    "    y = tst_model(x)\n",
    "    test_eq(h.stored[0], layers[0](x))\n",
    "    test_eq(h.stored[1], F.relu(layers[0](x)))\n",
    "    test_eq(h.stored[2], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_outputs(modules, detach=True, cpu=False, grad=False)->Hooks:\n",
    "    \"Return `Hooks` that store activations of all `modules` in `self.stored`\"\n",
    "    return Hooks(modules, _hook_inner, detach=detach, cpu=cpu, is_forward=not grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activations stored are the gradients if `grad=True`, otherwise the output of `modules`. If `detach=True` they are detached from their history, and if `cpu=True`, they're put on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(5,10), nn.ReLU(), nn.Linear(10,3)]\n",
    "tst_model = nn.Sequential(*layers)\n",
    "x = torch.randn(4,5)\n",
    "with hook_outputs(layers) as h:\n",
    "    y = tst_model(x)\n",
    "    test_eq(h.stored[0], layers[0](x))\n",
    "    test_eq(h.stored[1], F.relu(layers[0](x)))\n",
    "    test_eq(h.stored[2], y)\n",
    "    for s in h.stored: assert not s.requires_grad\n",
    "    \n",
    "with hook_outputs(layers, grad=True) as h:\n",
    "    y = tst_model(x)\n",
    "    loss = y.pow(2).mean()\n",
    "    loss.backward()\n",
    "    g = 2*y / y.numel()\n",
    "    test_close(g, h.stored[2][0])\n",
    "    g = g @ layers[2].weight.data\n",
    "    test_close(g, h.stored[1][0])\n",
    "    g = g * (layers[0](x) > 0).float()\n",
    "    test_close(g, h.stored[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda\n",
    "with hook_outputs(tst_model, cpu=True) as h:\n",
    "    y = tst_model.cuda()(x.cuda())\n",
    "    for s in h.stored: test_eq(s.device, torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HookCallback -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make hooks easy to use, we wrapped a version in a Callback where you just have to implement a `hook` function (plus any element you might need)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HookCallback(Callback):\n",
    "    \"`Callback` that can be used to register hooks on `modules`\"\n",
    "    def __init__(self, modules=None, do_remove=True, is_forward=True, detach=True, cpu=False):\n",
    "        self.modules,self.do_remove = modules,do_remove\n",
    "        self.is_forward,self.detach,self.cpu = is_forward,detach,cpu\n",
    "\n",
    "    def begin_fit(self):\n",
    "        \"Register the `Hooks` on `self.modules`.\"\n",
    "        if not self.modules:\n",
    "            self.modules = [m for m in flatten_model(self.model) if hasattr(m, 'weight')]\n",
    "        self.hooks = Hooks(self.modules, self.hook, self.is_forward, self.detach, self.cpu)\n",
    "\n",
    "    def after_fit(self):\n",
    "        \"Remove the `Hooks`.\"\n",
    "        if self.do_remove: self._remove()\n",
    "\n",
    "    def _remove(self): \n",
    "        if getattr(self, 'hooks', None): self.hooks.remove()\n",
    "    \n",
    "    def __del__(self): self._remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not provided, `modules` will default to the layers of `self.model` that have a `weight` attribute. Depending on `do_remove`, the hooks will be properly removed at the end of training (or in case of error). `is_forward` , `detach` and `cpu` are passed to `Hooks`.\n",
    "\n",
    "The function called at each forward (or backward) pass is `self.hook` and must be implemented when subclassing this callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(5.5230), tensor(3.3197), '00:00']\n"
     ]
    }
   ],
   "source": [
    "class TstCallback(HookCallback):\n",
    "    def hook(self, m, i, o): return o\n",
    "    def after_batch(self): test_eq(self.hooks.stored[0], self.pred)\n",
    "        \n",
    "learn = synth_learner(n_trn=5, cbs = TstCallback())\n",
    "learn.model = nn.Linear(1,1)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(14.1785), tensor(6.8552), '00:00']\n"
     ]
    }
   ],
   "source": [
    "class TstCallback(HookCallback):\n",
    "    def __init__(self, modules=None, do_remove=True, detach=True, cpu=False):\n",
    "        super().__init__(modules, do_remove, False, detach, cpu)\n",
    "    def hook(self, m, i, o): return o\n",
    "    def after_batch(self): \n",
    "        pass#TODO: fix\n",
    "        #test_eq(self.hooks.stored[0][0], 2*(self.pred-self.yb)/self.pred.shape[0])\n",
    "        \n",
    "learn = synth_learner(n_trn=5, cbs = TstCallback())\n",
    "learn.model = nn.Linear(1,1)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>HookCallback.begin_fit</code>\" class=\"doc_header\"><code>HookCallback.begin_fit</code><a href=\"https://github.com/fastai/fastai_docs/tree/master/dev/__main__.py#L7\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HookCallback.begin_fit</code>()\n",
       "\n",
       "Register the `Hooks` on `self.modules`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HookCallback.begin_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"<code>HookCallback.after_fit</code>\" class=\"doc_header\"><code>HookCallback.after_fit</code><a href=\"https://github.com/fastai/fastai_docs/tree/master/dev/__main__.py#L13\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HookCallback.after_fit</code>()\n",
       "\n",
       "Remove the `Hooks`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HookCallback.after_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of such a `HookCallback` is the following, that stores the mean and stds of activations that go through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@docs\n",
    "class ActivationStats(HookCallback):\n",
    "    \"Callback that record the mean and std of activations.\"\n",
    "\n",
    "    def begin_fit(self):\n",
    "        \"Initialize stats.\"\n",
    "        super().begin_fit()\n",
    "        self.stats = []\n",
    "\n",
    "    def hook(self, m, i, o): return o.mean().item(),o.std().item()\n",
    "    \n",
    "    def after_batch(self):\n",
    "        \"Take the stored results and puts it in `self.stats`\"\n",
    "        if self.training: self.stats.append(self.hooks.stored)\n",
    "    \n",
    "    def after_fit(self):\n",
    "        \"Polish the final result.\"\n",
    "        self.stats = tensor(self.stats).permute(2,1,0)\n",
    "        super().after_fit()\n",
    "        \n",
    "    _docs = dict(hook=\"Take the mean and std of the output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
